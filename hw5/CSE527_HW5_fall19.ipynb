{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "CSE527_HW5_fall19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcamCuuyjYb2"
      },
      "source": [
        "# Action Recognition @ UCF101  \n",
        "**Due date: 11:59 pm on Nov. 19, 2019 (Tuesday)**\n",
        "\n",
        "## Description\n",
        "---\n",
        "In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the amount of data. Consequently, a training sample is an image tuple that forms a 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n",
        "\n",
        "To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n",
        "\n",
        "Instead of training an end-to-end neural network from scratch whose computation is prohibitively expensive, we divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n",
        "- **{35 pts} Feature extraction**. Use any of the [pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html) to extract features from each frame. Specifically, we recommend not to use the activations of the last layer as the features tend to be task specific towards the end of the network. \n",
        "    **hints**: \n",
        "    - A good starting point would be to use a pre-trained VGG16 network, we suggest first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n",
        "    - Normalize your images using `torchvision.transforms` \n",
        "    ```\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "    prep(img)\n",
        "    The mean and std. mentioned above is specific to Imagenet data\n",
        "    \n",
        "    ```\n",
        "    More details of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    \n",
        "- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a **dx25** sample as input (where **d** is the dimension of the extracted feature for each frame), and outputs the action label of that sample.\n",
        "- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy **(5 points)**. The baseline test accuracy for this data is 75%, and **10 points** out of 20 is for achieving test accuracy greater than the baseline. Moreover, you need to compare **(5 points)** the result of your network with that of support vector machine (SVM) (stacking the **dx25** feature matrix to a long vector and train a SVM).\n",
        "- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n",
        "\n",
        "Notice that the size of the raw images is 256x340, whereas your pre-trained model might take **nxn** images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five **nxn** images, one at the image center and four at the corners and compute the **d**-dim features for each of them, and average these five **d**-dim feature to get a final feature representation for the raw image.\n",
        "For example, VGG takes 224x224 images as inputs, so we take the five 224x224 croppings of the image, compute 4096-dim VGG features for each of them, and then take the mean of these five 4096-dim vectors to be the representation of the image.\n",
        "\n",
        "In order to save you computational time, you need to do the classification task only for **the first 25** classes of the whole dataset. The same applies to those who have access to GPUs. **Bonus 10 points for running and reporting on the entire 101 classes.**\n",
        "\n",
        "\n",
        "## Dataset\n",
        "Download **dataset** at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar)(Image data for each video) and the **annos folder** which has the video labels and the label to class name mapping is included in the assignment folder uploaded. \n",
        "\n",
        "\n",
        "UCF101 dataset contains 101 actions and 13,320 videos in total.  \n",
        "\n",
        "+ `annos/actions.txt`  \n",
        "  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n",
        "  \n",
        "+ `annots/videos_labels_subsets.txt`  \n",
        "  + lists all the videos (`v_000001`, .., `v_013320`)  \n",
        "  + labels (`1`, .., `101`)  \n",
        "  + subsets (`1` for train, `2` for test)  \n",
        "\n",
        "+ `images/`  \n",
        "  + each folder represents a video\n",
        "  + the video/folder name to class mapping can be found using `annots/videos_labels_subsets.txt`, for e.g. `v_000001` belongs to class 1 i.e. `ApplyEyeMakeup`\n",
        "  + each video folder contains 25 frames  \n",
        "\n",
        "\n",
        "\n",
        "## Some Tutorials\n",
        "- Good materials for understanding RNN and LSTM\n",
        "    - http://blog.echen.me\n",
        "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- Implementing RNN and LSTM with PyTorch\n",
        "    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n",
        "    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpsuLr6cMdG_"
      },
      "source": [
        "## Libraries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic0aHoWJsAT0"
      },
      "source": [
        "from google.colab import drive\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import cv2\n",
        "import os\n",
        "from scipy.io import savemat\n",
        "from scipy.io import loadmat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8GhAb-nZTq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "aa338199-1ce5-4c55-a057-b1bc60a9d58e"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GpASMevZ-oZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "338e50db-300d-4375-ba03-a3a18d1299d1"
      },
      "source": [
        "cd '/content/drive/My Drive/CV/Homework/HW5/CSE527-HW5-Fall19'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CV/Homework/HW5/CSE527-HW5-Fall19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwvqn2c_LG7S"
      },
      "source": [
        "%%capture\n",
        "!tar -xvf  'dataset/Test2/UCF101_images.tar' -C '/content/drive/My Drive/CV/Homework/HW5/CSE527-HW5-Fall19/dataset/Test2/img'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRPulKP9qhHO"
      },
      "source": [
        "filepath=\"http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar\"\n",
        "newfilepath=\"dataset/Test2/UCF101_images.tar\"\n",
        "\n",
        "file_url = filepath\n",
        "\n",
        "print(file_url)\n",
        "r = requests.get(file_url, stream = True)  \n",
        "\n",
        "with open(newfilepath, \"wb\") as file:  \n",
        "    for block in r.iter_content(chunk_size = 1024): \n",
        "        if block:  \n",
        "            file.write(block)\n",
        "print(newfilepath)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bofOtvlU4QM8"
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"dataset/Test2/UCF101_images.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-HhuP46BuG"
      },
      "source": [
        "%%time\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pII4OpLyS2Dv"
      },
      "source": [
        "%%capture\n",
        "# !tar -xkf  'dataset/Test2/UCF101_images.tar' -C '/content/drive/My Drive/CV/Homework/HW5/CSE527-HW5-Fall19/dataset/Test2/img'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLRilCQhUjN"
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"dataset/Test2/UCF101_images.tar\", \"r:\")\n",
        "tar.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__5Cer_qwbHF"
      },
      "source": [
        "\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdAFfFsG2ShA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPAbsDy2Ssq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6a0288a3-824d-4652-85e0-0b4af8392960"
      },
      "source": [
        "# \\*write your codes for feature extraction (You can use multiple cells, this is just a place holder)\n",
        "%%time\n",
        "with open(\"annos/videos_labels_subsets.txt\") as f:\n",
        "    content = f.readlines()\n",
        "content = [x.strip() for x in content] \n",
        "video_names=[]\n",
        "labels=[]\n",
        "data_use=[]\n",
        "for c in content:\n",
        "  temp=c.split('\\t')\n",
        "  video_names.append(temp[0])\n",
        "  labels.append(temp[1])\n",
        "  data_use.append(temp[2])\n",
        "\n",
        "train_video_names=[video_names[i] for i in range(len(data_use)) if data_use[i] is '1' and int(labels[i]) in range(1, 102)]\n",
        "train_video_labels=[int(labels[i])-1 for i in range(len(data_use)) if data_use[i] is '1' and int(labels[i]) in range(1, 102)]\n",
        "\n",
        "test_video_names=[video_names[i] for i in range(len(data_use)) if data_use[i] is '2' and int(labels[i]) in range(1, 102)]\n",
        "test_video_labels=[int(labels[i])-1 for i in range(len(data_use)) if data_use[i] is '2' and int(labels[i]) in range(1, 102)]\n",
        "\n",
        "print(\"Number of training videos:\", len(train_video_labels))\n",
        "print(\"Number of testing videos:\", len(test_video_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training videos: 9537\n",
            "Number of testing videos: 3783\n",
            "CPU times: user 24.6 ms, sys: 2.57 ms, total: 27.2 ms\n",
            "Wall time: 948 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvMqaqu1yaUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5651736c-a47d-4fdb-9558-99cd56a63a66"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "#extract first fully connected layer\n",
        "first_fully_connected_vgg16 = nn.Sequential(*list(vgg16.classifier.children())[:1])\n",
        "vgg16.classifier=first_fully_connected_vgg16\n",
        "\n",
        "#Set up GPU\n",
        "device = torch.device(\"cuda:0\")\n",
        "vgg16.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 85.4MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.02 s, sys: 2.18 s, total: 6.2 s\n",
            "Wall time: 13.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHkf_3V92StN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a2f0e4c8-7fb4-42d6-e3a2-74b6ff51d67c"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from scipy.io import savemat\n",
        "from scipy.io import loadmat\n",
        "\n",
        "def crop_2_224x224_and_form_tensors(img):\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "  c=[]\n",
        "  c.append(prep(img[0:224, 0:224]))  \n",
        "  c.append(prep(img[32:256, 0:224]))\n",
        "  c.append(prep(img[0:224, 32:256]))\n",
        "  c.append(prep(img[32:256, 32:256]))\n",
        "  c.append(prep(img[16:240, 16:240]))\n",
        "  return c\n",
        "\n",
        "\n",
        "def getClassBatch(start_vid,end_vid):\n",
        "  videos=[]\n",
        "  for index,vid_folder in enumerate(sorted(os.listdir(\"dataset/Test2/img/images/\"))):\n",
        "    if(index>=start_vid and index<end_vid):\n",
        "      vid=[]\n",
        "      for i in os.listdir('dataset/Test2/img/images/'+vid_folder):\n",
        "        x=cv2.imread('dataset/Test2/img/images/'+vid_folder+'/'+i)\n",
        "        vid.append(x)\n",
        "      videos.append(vid)\n",
        "      # print(index)\n",
        "      # print(vid_folder)\n",
        "    elif index==end_vid:\n",
        "      print(vid_folder)\n",
        "      break\n",
        "  return videos\n",
        "\n",
        "def save_vid_mat(videos,start_vid):\n",
        "  all_ftrs=[]\n",
        "  i=start_vid\n",
        "  # print(\"Generating features for\")\n",
        "  for video in videos:\n",
        "    # print(\"video\", i+1, '- dataset/mat/'+video_names[i]+'.mat')\n",
        "    mean_fetr=None\n",
        "    video_ftrs=[]\n",
        "    ftr_data={}\n",
        "    for frame in video:\n",
        "      crops=crop_2_224x224_and_form_tensors(frame)\n",
        "      fetrs=[]\n",
        "      for crop in crops:\n",
        "        fetrs.append(vgg16(crop.unsqueeze(0).to(device)))\n",
        "      fetrs=torch.stack(fetrs)\n",
        "      mean_fetr=torch.mean(fetrs.cpu(), 0).detach().numpy()[0].tolist()\n",
        "      video_ftrs.append(mean_fetr)\n",
        "    ftr_data['Feature']=video_ftrs\n",
        "    savemat('dataset/mat/'+video_names[i]+'.mat', ftr_data)\n",
        "    i+=1\n",
        "  print(\"video\", i, '- dataset/mat/'+video_names[i-1]+'.mat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24 µs, sys: 0 ns, total: 24 µs\n",
            "Wall time: 26.7 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2VGnwv52StW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d2bb1564-add8-4686-acd4-12b96fa184e3"
      },
      "source": [
        "start_vid=4500\n",
        "end_vid=5000\n",
        "videos=getClassBatch(start_vid,end_vid)\n",
        "save_vid_mat(videos,start_vid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v_005001\n",
            "video 5000 - dataset/mat/v_005000.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCRwkL7jxtc"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 1.** Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urKQi8oAjYb-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5e1e45a5-1f09-4b48-bf37-d6826f064149"
      },
      "source": [
        "# \\*write your codes for feature extraction (You can use multiple cells, this is just a place holder)\n",
        "%%time\n",
        "with open(\"annos/videos_labels_subsets.txt\") as f:\n",
        "    content = f.readlines()\n",
        "content = [x.strip() for x in content] \n",
        "video_names=[]\n",
        "labels=[]\n",
        "data_use=[]\n",
        "for c in content:\n",
        "  temp=c.split('\\t')\n",
        "  video_names.append(temp[0])\n",
        "  labels.append(temp[1])\n",
        "  data_use.append(temp[2])\n",
        "\n",
        "train_video_names=[video_names[i] for i in range(len(data_use)) if data_use[i] is '1' and int(labels[i]) in range(1, 26)]\n",
        "train_video_labels=[int(labels[i])-1 for i in range(len(data_use)) if data_use[i] is '1' and int(labels[i]) in range(1, 26)]\n",
        "\n",
        "test_video_names=[video_names[i] for i in range(len(data_use)) if data_use[i] is '2' and int(labels[i]) in range(1, 26)]\n",
        "test_video_labels=[int(labels[i])-1 for i in range(len(data_use)) if data_use[i] is '2' and int(labels[i]) in range(1, 26)]\n",
        "\n",
        "print(\"Number of training videos:\", len(train_video_labels))\n",
        "print(\"Number of testing videos:\", len(test_video_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training videos: 2409\n",
            "Number of testing videos: 951\n",
            "CPU times: user 25.4 ms, sys: 140 µs, total: 25.5 ms\n",
            "Wall time: 723 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mR44zVGMAcp"
      },
      "source": [
        "# !ls dataset/images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsG9b-5qKzgJ"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "videos=[]\n",
        "for index,vid_folder in enumerate(sorted(os.listdir(\"dataset/images/\"))):\n",
        "  print(index)\n",
        "  if index==25:\n",
        "    break\n",
        "  # print(vid_folder)\n",
        "  vid=[]\n",
        "  for i in os.listdir('dataset/images/'+vid_folder):\n",
        "    x=cv2.imread('dataset/images/'+vid_folder+'/'+i)\n",
        "    vid.append(x)\n",
        "  videos.append(vid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m5pCsI6o1L9"
      },
      "source": [
        "print(len(videos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1YX5T5qqkiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "810f89f8-1404-4b36-92a6-43a0572f904b"
      },
      "source": [
        "#Download pre trained VGG16 model\n",
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "#extract first fully connected layer\n",
        "first_fully_connected_vgg16 = nn.Sequential(*list(vgg16.classifier.children())[:1])\n",
        "vgg16.classifier=first_fully_connected_vgg16\n",
        "\n",
        "#Set up GPU\n",
        "device = torch.device(\"cuda:0\")\n",
        "vgg16.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:22<00:00, 24.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.8 s, sys: 2.34 s, total: 7.14 s\n",
            "Wall time: 31.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA3Vxu0orips"
      },
      "source": [
        "%%capture\n",
        "def crop_2_224x224_and_form_tensors(img):\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
        "  c=[]\n",
        "  c.append(prep(img[0:224, 0:224]))  \n",
        "  c.append(prep(img[32:256, 0:224]))\n",
        "  c.append(prep(img[0:224, 32:256]))\n",
        "  c.append(prep(img[32:256, 32:256]))\n",
        "  c.append(prep(img[16:240, 16:240]))\n",
        "  return c\n",
        "\n",
        "\n",
        "print(\"Reading video frames\")\n",
        "# class1_videos=[]\n",
        "\n",
        "videos=[]\n",
        "for index,vid_folder in enumerate(sorted(os.listdir(\"dataset/images/\"))):\n",
        "  print(index)\n",
        "  if index==3360:\n",
        "    print(vid_folder)\n",
        "    break\n",
        "  vid=[]\n",
        "  for i in os.listdir('dataset/images/'+vid_folder):\n",
        "    x=cv2.imread('dataset/images/'+vid_folder+'/'+i)\n",
        "    vid.append(x)\n",
        "  videos.append(vid)\n",
        "# class1_videos=[[cv2.imread('dataset/images/'+vid_folder+'/'+i) for i in os.listdir('dataset/images/'+vid_folder)] for vid_folder in enumerate(os.listdir(\"dataset/images/\"))]\n",
        "print(\"Finished reading video frames\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snoJCKDtGhzr"
      },
      "source": [
        "print(len(videos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwNAHfkMJ4om"
      },
      "source": [
        "%%capture\n",
        "all_ftrs=[]\n",
        "i=0\n",
        "print(\"Generating features for\")\n",
        "for video in videos:\n",
        "  print(\"video\", i+1, '- dataset/mat/'+video_names[i]+'.mat')\n",
        "  mean_fetr=None\n",
        "  video_ftrs=[]\n",
        "  ftr_data={}\n",
        "  for frame in video:\n",
        "    crops=crop_2_224x224_and_form_tensors(frame)\n",
        "    fetrs=[]\n",
        "    for crop in crops:\n",
        "      fetrs.append(vgg16(crop.unsqueeze(0).to(device)))\n",
        "    fetrs=torch.stack(fetrs)\n",
        "    mean_fetr=torch.mean(fetrs.cpu(), 0).detach().numpy()[0].tolist()\n",
        "    video_ftrs.append(mean_fetr)\n",
        "  ftr_data['Feature']=video_ftrs\n",
        "  savemat('dataset/mat/'+video_names[i]+'.mat', ftr_data)\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg89tRTIWoce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "707f85c8-6d58-413d-fde0-d53998468f9e"
      },
      "source": [
        "print(len(train_video_names))\n",
        "print(len(test_video_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2409\n",
            "951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uBOo5O8Evl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3376c910-8285-43bb-f004-ab8284510a67"
      },
      "source": [
        "print(\"Reading .mat files\")\n",
        "train_video_features=[loadmat('dataset/mat/'+vid) for vid in train_video_names]\n",
        "print(\"loaded training features\")\n",
        "test_video_features=[loadmat('dataset/mat/'+vid) for vid in test_video_names]\n",
        "print(\"loaded testing features\")\n",
        "print(\"Finished reading .mat files\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading .mat files\n",
            "loaded training features\n",
            "loaded testing features\n",
            "Finished reading .mat files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN74WLWpl7zQ"
      },
      "source": [
        "***\n",
        "***\n",
        "## **Problem 2.** Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do5OSV9Mnmwy"
      },
      "source": [
        "* ##### **Print the size of your training and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EGU31IJn5_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4671dcda-fc56-4140-d38a-cdf5b8bebd0f"
      },
      "source": [
        "# Don't hardcode the shape of train and test data\n",
        "print('Shape of training data is :', train_video_features[0]['Feature'].shape)\n",
        "print('Shape of test/validation data is :', test_video_features[0]['Feature'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data is : (25, 4096)\n",
            "Shape of test/validation data is : (25, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqshyjO3mHkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "outputId": "90357fd4-f850-4191-a401-6f4271063cbd"
      },
      "source": [
        "# \\*write your codes for modelling using the extracted feature (You can use multiple cells, this is just a place holder)\n",
        "class ActionRecognition(nn.Module):\n",
        "  def __init__(self):\n",
        "    \n",
        "    super(ActionRecognition, self).__init__()\n",
        "    \n",
        "    self.recurrent_layer = torch.nn.LSTM(4096, 1000, 1)\n",
        "    self.project_layer = torch.nn.Linear(1000, 25)\n",
        "    \n",
        "    '''\n",
        "    #Alternate model\n",
        "    self.project_layer=torch.nn.Linear(4096, 100)\n",
        "    self.recurr_layer=torch.nn.LSTM(100, 1000, 1)\n",
        "    self.dropout_layer=torch.nn.Dropout(p=0.5)\n",
        "    self.classify=torch.nn.Linear(1000, 15)\n",
        "    '''\n",
        "    \n",
        "  def forward(self, input):\n",
        "    \n",
        "    rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
        "    logits = self.project_layer(rnn_outputs[:,-1])\n",
        "    \n",
        "    '''\n",
        "    #Alternate model\n",
        "    proj=self.project_layer(input)\n",
        "    rnn_outputs, (hn, cn) = self.recurr_layer(proj)\n",
        "    drpout=self.dropout_layer(rnn_outputs[:, 1])\n",
        "    logits = self.classify(drpout)\n",
        "    '''\n",
        "    \n",
        "    return logits\n",
        "\n",
        "model = ActionRecognition()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)\n",
        "\n",
        "n_epoch=30\n",
        "\n",
        "for epoch in range(n_epoch):  \n",
        "  running_loss=0.0\n",
        "  for i in range(len(train_video_features)):    \n",
        "    \n",
        "    #inputs, labels=torch.tensor(train_video_features[i]['Feature']).unsqueeze(0), torch.tensor([train_labels[i]])\n",
        "    inputs, labels=Variable(torch.tensor(train_video_features[i]['Feature']).unsqueeze(0)), Variable(torch.tensor([train_video_labels[i]]))\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs.float())\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss+=loss.item()\n",
        "    \n",
        "  print(\"epoch:\", epoch+1, \"----> Average loss:\", running_loss/float(len(train_video_features)))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ActionRecognition(\n",
            "  (recurrent_layer): LSTM(4096, 1000)\n",
            "  (project_layer): Linear(in_features=1000, out_features=25, bias=True)\n",
            ")\n",
            "epoch: 1 ----> Average loss: 0.18225095407979702\n",
            "epoch: 2 ----> Average loss: 0.09481456150268809\n",
            "epoch: 3 ----> Average loss: 0.057404339215594256\n",
            "epoch: 4 ----> Average loss: 0.037582505137281515\n",
            "epoch: 5 ----> Average loss: 0.029437667363568076\n",
            "epoch: 6 ----> Average loss: 0.025121950728011003\n",
            "epoch: 7 ----> Average loss: 0.021359364879323443\n",
            "epoch: 8 ----> Average loss: 0.018146090203477433\n",
            "epoch: 9 ----> Average loss: 0.01601811330089266\n",
            "epoch: 10 ----> Average loss: 0.013497788622909802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7f9dcd3df464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkzVxXSOx0um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "21159afc-9713-485d-ccfc-de057935b70d"
      },
      "source": [
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_video_features)):\n",
        "        images, labels = torch.tensor(test_video_features[i]['Feature']).unsqueeze(0), torch.tensor([test_video_labels[i]])\n",
        "        outputs = model(images.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "print(\"\\nAccuracy : \"+str(100 * correct / float(total))+\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy : 35.75184016824395%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0znm2TMmsDZ"
      },
      "source": [
        "---\n",
        "---\n",
        "## **Problem 3.** Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtMhcbXmsXk"
      },
      "source": [
        "# \\*write your codes for evaluation (You can use multiple cells, this is just a place holder)\n",
        "\n",
        "print(\"Generating long vectors for SVM\\n\")\n",
        "\n",
        "#Stacking up training features\n",
        "train_long_vectors=[np.asarray(video_ftrs['Feature']).flatten('F') for video_ftrs in train_video_features]\n",
        "\n",
        "#stacking up testing features\n",
        "test_long_vectors=[np.asarray(video_ftrs['Feature']).flatten('F') for video_ftrs in test_video_features]\n",
        "\n",
        "print(\"starting SVM training\")\n",
        "svm_train_start_time=time.time()\n",
        "clf = LinearSVC(random_state=0, tol=1e-5)\n",
        "clf.fit(train_long_vectors, train_video_labels)\n",
        "svm_train_end_time=time.time()\n",
        "print(\"Total time for training: \"+str(((svm_train_end_time-svm_train_start_time))/float(60))+\" mins\")\n",
        "\n",
        "print(\"\\nSVM testing\")\n",
        "svm_test_start_time=time.time()\n",
        "pred = clf.predict(test_long_vectors)\n",
        "accuracy=np.mean(pred==test_video_labels)*100\n",
        "svm_test_end_time=time.time()\n",
        "print(\"Total time for testing: \"+str(svm_test_end_time-svm_test_start_time)+\" secs\")\n",
        "print(\"\\nAccuracy : \"+str(accuracy)+\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ZUeqnGv48f"
      },
      "source": [
        "* ##### **Print the train and test accuracy of your model** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMMXAjMjv4g5"
      },
      "source": [
        "# Don't hardcode the train and test accuracy\n",
        "print('Training accuracy is %2.3f :' %(100.00) )\n",
        "print('Test accuracy is %2.3f :' %(100.00) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eesNQn6FYKQz"
      },
      "source": [
        "* ##### **Print the train and test and test accuracy of SVM** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip87hPqTYJtr"
      },
      "source": [
        "# Don't hardcode the train and test accuracy\n",
        "print('Training accuracy is %2.3f :' %(100.00) )\n",
        "print('Test accuracy is %2.3f :' %(100.00) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL4Y3nHBkwmb"
      },
      "source": [
        "## **Problem 4.** Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbzpDeRRNuHj"
      },
      "source": [
        "## **Bonus**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPrhLzuyN-rr"
      },
      "source": [
        "* ##### **Print the size of your training and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IR6zrwOENz"
      },
      "source": [
        "# Don't hardcode the shape of train and test data\n",
        "print('Shape of training data is :', )\n",
        "print('Shape of test/validation data is :', )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAakVg8-OE_j"
      },
      "source": [
        "* ##### **Modelling and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHbPzkcoObLb"
      },
      "source": [
        "#Write your code for modelling and evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb4Wlzw2jYcJ"
      },
      "source": [
        "## Submission\n",
        "---\n",
        "**Runnable source code in ipynb file and a pdf report are required**.\n",
        "\n",
        "The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n",
        "\n",
        "Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts.\n",
        "The naming convention for report is **Surname_Givenname_SBUID_report*.pdf**\n",
        "\n",
        "When submitting your .zip file through blackboard, please\n",
        "-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n",
        "\n",
        "This zip file should include:\n",
        "```\n",
        "Surname_Givenname_SBUID_hw*\n",
        "        |---Surname_Givenname_SBUID_hw*.ipynb\n",
        "        |---Surname_Givenname_SBUID_hw*.pdf\n",
        "        |---Surname_Givenname_SBUID_report*.pdf\n",
        "```\n",
        "\n",
        "For instance, student Michael Jordan should submit a zip file named \"Jordan_Michael_111134567_hw5.zip\" for homework5 in this structure:\n",
        "```\n",
        "Jordan_Michael_111134567_hw5\n",
        "        |---Jordan_Michael_111134567_hw5.ipynb\n",
        "        |---Jordan_Michael_111134567_hw5.pdf\n",
        "        |---Jordan_Michael_111134567_report*.pdf\n",
        "```\n",
        "\n",
        "The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link**. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. \n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter two TA's emails: ***bo.cao.1@stonybrook.edu*** and ***sayontan.ghosh@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
        "\n",
        "Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n",
        "\n",
        "You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."
      ]
    }
  ]
}
